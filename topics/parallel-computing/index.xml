<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Parallel Computing on See the Word Abstractly</title>
    <link>https://constroy.github.io/topics/parallel-computing/</link>
    <description>Recent content in Parallel Computing on See the Word Abstractly</description>
    <generator>Hugo -- gohugo.io</generator>
    <copyright>&amp;copy; 2016. All rights reserved.</copyright>
    <lastBuildDate>Wed, 13 Apr 2016 22:21:19 +0800</lastBuildDate>
    <atom:link href="https://constroy.github.io/topics/parallel-computing/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>MPI点到点通信实验报告</title>
      <link>https://constroy.github.io/post/MPI%E7%82%B9%E5%88%B0%E7%82%B9%E9%80%9A%E4%BF%A1%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A/</link>
      <pubDate>Wed, 13 Apr 2016 22:21:19 +0800</pubDate>
      
      <guid>https://constroy.github.io/post/MPI%E7%82%B9%E5%88%B0%E7%82%B9%E9%80%9A%E4%BF%A1%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A/</guid>
      <description>

&lt;h3 id=&#34;实验要求&#34;&gt;实验要求&lt;/h3&gt;

&lt;p&gt;使用MPI的点对点通信，将数据从进程0传出，以环型方式（Ring），依次传输给所有的进程（最后传到进程0），也可以说，以接力的方式广播数据，计算数据传输一圈的时间。同时计算每个节点上传输一圈的时间。&lt;/p&gt;

&lt;h3 id=&#34;设计方法&#34;&gt;设计方法&lt;/h3&gt;

&lt;p&gt;从0号进程开始，将当前时间作为数据传给下一个进程。每个进程收到上个进程发来的时间戳，用当前时间减之，得到这次传输的时间。再把当前发给下一个进程。直到0号进程收到N - 1号进程的数据，完成一圈。0号进程用当前减开始时间，得到一圈的总用时（包含每个进程自己占用的时间）。&lt;/p&gt;

&lt;p&gt;重复R次，用总时间除以R，得出平均时间。R可由程序参数指定（通过mpirun传递参数）。&lt;/p&gt;

&lt;p&gt;主要使用的函数：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;MPI_Comm_rank()     // 获取本进程ID
MPI_Comm_size()     // 获取总进程数
MPI_Wtime()         // 获取从某时刻开始经过的时间
MPI_Send()          // 阻塞地发出数据
MPI_Recv()          // 阻塞地接收数据
MPI_Barrier()       // 阻塞直到每一个进程都到达这个函数调用
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;实现代码&#34;&gt;实现代码&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;#include &amp;lt;stdlib.h&amp;gt;
#include &amp;lt;stdio.h&amp;gt;
#include &amp;lt;mpi.h&amp;gt;

int main(int argc, char *argv[]) {
	int i, round, myid, numprocs;
	double begin, now, then, total = .0, sum = .0;
	MPI_Status status;

	/* just one arg, the number of rounds is needed */
	if (argc != 2) {
		puts(&amp;quot;too many/few args&amp;quot;);
		return 0;
	}

	/* get the number of rounds from argv */
	round = atoi(argv[1]);

	MPI_Init(&amp;amp;argc, &amp;amp;argv);
	MPI_Comm_rank(MPI_COMM_WORLD, &amp;amp;myid);
	MPI_Comm_size(MPI_COMM_WORLD, &amp;amp;numprocs);

	if (myid == 0) {
		for (i = 0; i &amp;lt; round; ++i) {
			/* take down the begin time */
			begin = now = MPI_Wtime();

			/* send the current time to next process */
			MPI_Send(&amp;amp;now, 1, MPI_DOUBLE, 1, i, MPI_COMM_WORLD);

			/* receive time stamp from previous process */
			MPI_Recv(&amp;amp;then, 1, MPI_DOUBLE, numprocs - 1, i, MPI_COMM_WORLD, &amp;amp;status);
			now = MPI_Wtime();

			/* accumulate the total time to this process */
			total += now - then;

			/* accumulate the time in all rounds */
			sum += now - begin;
		}

		/* print the time used from N - 1 to 0 */
		printf(&amp;quot;time used on %2d to %2d: %.6fs / %d rounds\n&amp;quot;, numprocs - 1, myid, total, round);
	} else {
		for (i = 0; i &amp;lt; round; ++i) {
			/* receive time stamp from previous process */
			MPI_Recv(&amp;amp;then, 1, MPI_DOUBLE, myid - 1, i, MPI_COMM_WORLD, &amp;amp;status);
			/* accumulate the total time to this process */
			total += MPI_Wtime() - then;

			/* send the current time to next process */
			now = MPI_Wtime();
			MPI_Send(&amp;amp;now, 1, MPI_DOUBLE, (myid + 1) % numprocs, i, MPI_COMM_WORLD);
		}
		/* print the time used from id - 1 to id */
		printf(&amp;quot;time used on %2d to %2d: %.6fs / %d rounds\n&amp;quot;, myid - 1, myid, total, round);
	}

	/* wait for processes to complete calculating */
	MPI_Barrier(MPI_COMM_WORLD);

	/* print the total time used in a round */
	if (myid == 0)
		printf(&amp;quot;total time used in a round: %.6fs / %d rounds\n&amp;quot;, sum, round);

	MPI_Finalize();
	return 0;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;运行结果&#34;&gt;运行结果&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ cat ring.qsub.o105
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;time used on  8 to  9: 1.230435s / 1000000 rounds
time used on  9 to 10: 1.322079s / 1000000 rounds
time used on 10 to 11: 1.496474s / 1000000 rounds
time used on 11 to 12: 1.347279s / 1000000 rounds
time used on 12 to 13: 0.822514s / 1000000 rounds
time used on 13 to 14: 1.267359s / 1000000 rounds
time used on 14 to 15: 1.265776s / 1000000 rounds
time used on 15 to  0: 0.774339s / 1000000 rounds
total time used in a round: 19.242893s / 1000000 rounds
time used on  0 to  1: 0.906652s / 1000000 rounds
time used on  1 to  2: 1.471215s / 1000000 rounds
time used on  2 to  3: 0.869112s / 1000000 rounds
time used on  3 to  4: 0.846962s / 1000000 rounds
time used on  4 to  5: 0.879961s / 1000000 rounds
time used on  5 to  6: 1.331113s / 1000000 rounds
time used on  6 to  7: 1.247567s / 1000000 rounds
time used on  7 to  8: 1.236116s / 1000000 rounds
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;遇到的问题&#34;&gt;遇到的问题&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;不清楚&lt;code&gt;MPI_Send()&lt;/code&gt;与&lt;code&gt;MPI_recv()&lt;/code&gt;函数中第二个参数&lt;code&gt;int count&lt;/code&gt;的含义，以为是数据大小，就传入了&lt;code&gt;sizeof now&lt;/code&gt;和&lt;code&gt;sizeof then&lt;/code&gt;，造成了缓冲区溢出。后来查文档知道了&lt;code&gt;count&lt;/code&gt;是数据的个数。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;不能保证一圈的总时间在各节点时间输出后输出。由于0号节点输出一圈总时间的时刻，其他进程可能还未输出该节点的时间。后来用&lt;code&gt;MPI_Barrier()&lt;/code&gt;使进度同步，但并没有解决这个问题，可能与输出缓冲区有关，需要避免多进程输出。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
  </channel>
</rss>